{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "source": [
    "# Objective\n",
    "To get an overview of all the possible feature engineering possible in this competition, I'll try to collect and benchmark everything I can find in other kernels and everything I come up with myself in this notebook. It'll be a work-in-progress as I do not have that much time on my hands as to do it in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc \n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import skew, kurtosis, gmean\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD, FastICA, NMF, FactorAnalysis\n",
    "from sklearn.decomposition import PCA, SparsePCA, MiniBatchSparsePCA, KernelPCA, IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.model_selection._split import check_cv\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41d836946df80423691f23bd307e739ac1a6a031"
   },
   "source": [
    "# 1. Data Loading & Pre-processing\n",
    "Current pre-processing pipeline follows learning from this notebook:<br />\n",
    "https://www.kaggle.com/c/santander-value-prediction-challenge/kernels\n",
    "\n",
    "Summary:\n",
    "* Log-transform all columns\n",
    "* Mean-variance scale all columns excepting sparse entries\n",
    "\n",
    "I'm not removing zero-variance and duplicate columns, since these are not constant/duplicates in test, and they could contain information when combined with other columns.\n",
    "\n",
    "* NOTE: I'm only sampling 500 rows from test and train, so as to experiment quicker in this kernel. I'm running it on the entire dataset locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7891baeb7f9a3eb1578a5b3ecea05834fe3aba37"
   },
   "outputs": [],
   "source": [
    "# Read train and test files\n",
    "train_df = pd.read_csv('../input/train.csv').sample(500)\n",
    "test_df = pd.read_csv('../input/test.csv').sample(500)\n",
    "\n",
    "# Get the combined data\n",
    "total_df = pd.concat([train_df.drop('target', axis=1), test_df], axis=0).drop('ID', axis=1)\n",
    "\n",
    "# Get the target\n",
    "y = np.log1p(train_df.target)\n",
    "\n",
    "# Log-transform all column\n",
    "total_df.loc[:, :] = np.log1p(total_df.values)\n",
    "\n",
    "# Scale non-zero column values\n",
    "for col in total_df.columns:        \n",
    "    nonzero_rows = total_df[col] != 0\n",
    "    if nonzero_rows.sum() > 0:\n",
    "        total_df.loc[nonzero_rows, col] = scale(total_df.loc[nonzero_rows, col].values)\n",
    "    \n",
    "# Train and test\n",
    "train_idx = range(0, len(train_df))\n",
    "test_idx = range(len(train_df), len(total_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "baa7a8420cc1362737fae2ad5d9532edec0af2de",
    "collapsed": true
   },
   "source": [
    "## 1.1. Aggregations and Functions\n",
    "Several aggregation features have been suggested, the ones I've read through:\n",
    "* https://www.kaggle.com/samratp/aggregates-sumvalues-sumzeros-k-means-pca\n",
    "* https://www.kaggle.com/mortido/digging-into-the-data-time-series-theory\n",
    "* https://www.kaggle.com/ianchute/geometric-mean-of-each-row-lb-1-55\n",
    "* https://www.kaggle.com/sggpls/pipeline-kernel-xgb-fe-lb1-39/code\n",
    "\n",
    "The code below is especially inspired by Sergey's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "54e8d445e046d600cd31dfe742f07cea436a8f18"
   },
   "outputs": [],
   "source": [
    "aggregate_df = pd.DataFrame()\n",
    "\n",
    "# Wrapper function\n",
    "def diff2(x):\n",
    "    return np.diff(x, n=2)\n",
    "\n",
    "# Different pre-processing to be used before each primary function\n",
    "preprocess_steps = [\n",
    "    [],\n",
    "    [np.diff], [diff2],\n",
    "    [np.unique], [np.unique, np.diff], [np.unique, diff2]    \n",
    "]\n",
    "\n",
    "# Different statistics to calculate on each preprocessed step\n",
    "stats = [len, np.min, np.max, np.median, np.std, skew, kurtosis] + 19 * [np.percentile]\n",
    "stats_kwargs = [{} for i in range(7)] + [{'q': np.round(i, 2)} for i in np.linspace(0.05, 0.95, 19)]\n",
    "\n",
    "# Only operate on non-nulls\n",
    "for funs in preprocess_steps:\n",
    "    \n",
    "    # Apply pre-processing steps\n",
    "    x = total_df[total_df != 0]\n",
    "    for f in funs:\n",
    "        x = f(x)\n",
    "        \n",
    "    # Go through our set of stat functions\n",
    "    for stat, stat_kwargs in zip(stats, stats_kwargs):\n",
    "        \n",
    "        # Construct feature name\n",
    "        name_components = [\n",
    "            stat.__name__,\n",
    "            \"_\".join([f.__name__ for f in funs]),\n",
    "            \"_\".join([\"{}={}\".format(k, v) for k,v in stat_kwargs.items()])\n",
    "        ]\n",
    "        feature_name = \"-\".join([e for e in name_components if e])\n",
    "\n",
    "        # Calc and save new feature in our dataframe\n",
    "        aggregate_df[feature_name] = total_df.apply(lambda x: stat(x, **stat_kwargs), axis=1)\n",
    "        \n",
    "# Extra features\n",
    "aggregate_df['number_of_different'] = total_df.nunique(axis=1)\n",
    "aggregate_df['non_zero_count'] = total_df.astype(bool).sum(axis=1) \n",
    "aggregate_df['sum_zeros'] = (total_df == 0).astype(int).sum(axis=1)\n",
    "aggregate_df['non_zero_fraction'] = total_df.shape[1] / total_df.astype(bool).sum(axis=1) \n",
    "aggregate_df['geometric_mean'] = total_df.apply(\n",
    "    lambda x: np.exp(np.log(x[x>0]).mean()), axis=1\n",
    ")\n",
    "aggregate_df.reset_index(drop=True, inplace=True)\n",
    "aggregate_df['geometric_mean'] = aggregate_df['geometric_mean'].replace(np.nan, 0)\n",
    "aggregate_df['non_zero_fraction'] = aggregate_df['non_zero_fraction'].replace(np.inf, 0)\n",
    "\n",
    "# Show user which aggregates were created\n",
    "print(f\">> Created {len(aggregate_df.columns)} features for; {aggregate_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acf6ca30388ac0b6432fb77cebe3c216d2eae13d"
   },
   "source": [
    "## 1.2. Decomposition Methods\n",
    "Lots of people have been using decomposition methods to reduce the number of features. The ones I've read through so far:\n",
    "* https://www.kaggle.com/shivamb/introduction-to-dataset-decomposition-techniques\n",
    "* https://www.kaggle.com/yekenot/baseline-with-decomposition-components\n",
    "\n",
    "From my trials in [this notebook](https://www.kaggle.com/nanomathias/linear-regression-with-elastic-net), it seems like often it's only the first 10-20 components that are actually important for the modeling. Since we are testing features now, here I'll include 10 of each decomposition method.\n",
    "\n",
    "* Note: some of the methods I only fit on training dataset, due to kernel limitations. I believe they should be fitted on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "663e156528c534a8c64a39fb21553627b678ae63"
   },
   "outputs": [],
   "source": [
    "COMPONENTS = 10\n",
    "\n",
    "# Convert to sparse matrix\n",
    "sparse_matrix = scipy.sparse.csr_matrix(total_df.values)\n",
    "\n",
    "# V1 List of decomposition methods\n",
    "methods = [\n",
    "    {'method': KernelPCA(n_components=2, kernel=\"rbf\"), 'data': 'train'},\n",
    "    {'method': FactorAnalysis(n_components=COMPONENTS), 'data': 'total'},\n",
    "    {'method': TSNE(n_components=3, init='pca'), 'data': 'train'},\n",
    "    {'method': TruncatedSVD(n_components=COMPONENTS), 'data': 'sparse'},\n",
    "    {'method': PCA(n_components=COMPONENTS), 'data': 'total'},\n",
    "    {'method': FastICA(n_components=COMPONENTS), 'data': 'total'},\n",
    "    {'method': GaussianRandomProjection(n_components=COMPONENTS, eps=0.1), 'data': 'total'},\n",
    "    {'method': SparseRandomProjection(n_components=COMPONENTS, dense_output=True), 'data': 'total'}\n",
    "]\n",
    "\n",
    "# Run all the methods\n",
    "embeddings = []\n",
    "for run in methods:\n",
    "    name = run['method'].__class__.__name__\n",
    "    \n",
    "    # Run method on appropriate data\n",
    "    if run['data'] == 'sparse':\n",
    "        embedding = run['method'].fit_transform(sparse_matrix)\n",
    "    elif run['data'] == 'train':\n",
    "        # NOTE: I do this due to memory limitations of the kaggle kernel\n",
    "        # locally I would use all the data.\n",
    "        embedding = run['method'].fit_transform(total_df.iloc[train_idx])\n",
    "    else:\n",
    "        embedding = run['method'].fit_transform(total_df)\n",
    "        \n",
    "    # Save in list of all embeddings\n",
    "    embeddings.append(\n",
    "        pd.DataFrame(embedding, columns=[f\"{name}_{i}\" for i in range(embedding.shape[1])])\n",
    "    )\n",
    "    print(f\">> Ran {name}\")\n",
    "    gc.collect()    \n",
    "    \n",
    "# Put all components into one dataframe\n",
    "components_df = pd.concat(embeddings, axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a57cbc94ac6bdb5e7fa53802f371fcb5aa26caf0"
   },
   "source": [
    "## 1.3. Dense Autoencoder\n",
    "I saw a few people use autoencoders, but here I just implement a very simple one. From empirical tests it seems that the components I extract from this it doesn't make sense to have an embedded dimension higher than about 5-10. I've tried tuning the dense autoencoder in terms of layers, dropout, batch normalization, and learning rate, but I usually do not get anything much better than the one presented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d0194d5f53a8b942290250896182ceb4587c0367"
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "enc_input = Input((total_df.shape[1], ))\n",
    "enc_output = Dense(512, activation='relu')(enc_input)\n",
    "enc_output = Dropout(0.5)(enc_output)\n",
    "enc_output = Dense(5, activation='relu')(enc_output)\n",
    "\n",
    "dec_input = Dense(512, activation='relu')(enc_output)\n",
    "dec_output = Dropout(0.5)(dec_input)\n",
    "dec_output = Dense(total_df.shape[1], activation='relu')(dec_output)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "vanilla_encoder = Model(enc_input, enc_output)\n",
    "vanilla_autoencoder = Model(enc_input, dec_output)\n",
    "vanilla_autoencoder.compile(optimizer=Adam(0.0001), loss='mean_squared_error')\n",
    "vanilla_autoencoder.summary()\n",
    "\n",
    "# Fit the autoencoder\n",
    "vanilla_autoencoder.fit(\n",
    "    total_df.values, total_df.values,\n",
    "    epochs=6, # INCREASE THIS ONE\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        ReduceLROnPlateau(monitor='loss', patience=5, verbose=1),\n",
    "        EarlyStopping(monitor='loss', patience=10, mode='min', min_delta=1e-5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Put into dataframe\n",
    "dense_ae_df = pd.DataFrame(\n",
    "    vanilla_encoder.predict(total_df.values, batch_size=256), \n",
    "    columns=['dense_AE_{}'.format(i) for i in range(5)]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "513753bf0c80e17f343f70304278d541df9adc36"
   },
   "source": [
    "## 1.4. Supervised Learning features\n",
    "The code for extracting these features is taken directly from Sergey's kernel:\n",
    "* https://www.kaggle.com/sggpls/pipeline-kernel-xgb-fe-lb1-39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0f5c8fa42d94face0eefe4cf59569e8dc5d14033"
   },
   "outputs": [],
   "source": [
    "# Define regressors and class-levels to go through\n",
    "classes = range(2, 7)\n",
    "regressors = [\n",
    "    ExtraTreesClassifier(\n",
    "        n_estimators=100, max_features=0.5,\n",
    "        max_depth=None, max_leaf_nodes=270,\n",
    "        min_impurity_decrease=0.0001,\n",
    "        n_jobs=-1, class_weight='balanced'\n",
    "    ),\n",
    "    LogisticRegression(\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "]\n",
    "\n",
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"https://www.kaggle.com/sggpls/pipeline-kernel-xgb-fe-lb1-39\"\"\"\n",
    "    \n",
    "    def __init__(self, estimator=None, n_classes=2, cv=3):\n",
    "        self.estimator = estimator\n",
    "        self.n_classes = n_classes\n",
    "        self.cv = cv\n",
    "    \n",
    "    def _get_labels(self, y):\n",
    "        y_labels = np.zeros(len(y))\n",
    "        y_us = np.sort(np.unique(y))\n",
    "        step = int(len(y_us) / self.n_classes)\n",
    "        \n",
    "        for i_class in range(self.n_classes):\n",
    "            if i_class + 1 == self.n_classes:\n",
    "                y_labels[y >= y_us[i_class * step]] = i_class\n",
    "            else:\n",
    "                y_labels[\n",
    "                    np.logical_and(\n",
    "                        y >= y_us[i_class * step],\n",
    "                        y < y_us[(i_class + 1) * step]\n",
    "                    )\n",
    "                ] = i_class\n",
    "        return y_labels\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y_labels = self._get_labels(y)\n",
    "        cv = check_cv(self.cv, y_labels, classifier=is_classifier(self.estimator))\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        for train, _ in cv.split(X, y_labels):\n",
    "            self.estimators_.append(\n",
    "                clone(self.estimator).fit(X[train], y_labels[train])\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n",
    "        \n",
    "        X_prob = np.zeros((X.shape[0], self.n_classes))\n",
    "        X_pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator, (_, test) in zip(self.estimators_, cv.split(X)):\n",
    "            X_prob[test] = estimator.predict_proba(X[test])\n",
    "            X_pred[test] = estimator.predict(X[test])\n",
    "        return np.hstack([X_prob, np.array([X_pred]).T])\n",
    "\n",
    "# Put all features into one dataframe (i.e. aggregate, timeseries, components)\n",
    "feature_df = pd.concat([components_df, aggregate_df, dense_ae_df], axis=1).fillna(0)    \n",
    "    \n",
    "# Collect predictions\n",
    "clf_features = []\n",
    "clf_columns = []\n",
    "for n in tqdm(classes):\n",
    "    for regr in regressors:\n",
    "        clf = ClassifierTransformer(regr, n_classes=n, cv=5)\n",
    "        clf.fit(feature_df.iloc[train_idx].values, y)\n",
    "        clf_features.append(\n",
    "            clf.transform(feature_df.values)\n",
    "        )\n",
    "        clf_columns += [f\"{n}-{regr.__class__.__name__}_pred{i}\" for i in range(n+1)]\n",
    "\n",
    "# Save into dataframe\n",
    "clf_features = np.concatenate(clf_features, axis=1)\n",
    "classifier_df = pd.DataFrame(clf_features, columns=clf_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3a9ddb93a08250e895ac8f5aff78764ca43758a"
   },
   "source": [
    "## 1.5. Time-series features\n",
    "Coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd5adc50079640b25ea58bd20d62a004c77e8a8c"
   },
   "source": [
    "# 2. Feature Benchmarking / Importance Testing\n",
    "To test these features, I'll probe them both individually and in combinations against the target value with local CV scores\n",
    "\n",
    "## 2.1. Individual Feature Testing\n",
    "Here I'm running 10-fold CV scores against target using one feature at a time, in order to see which features perform the best by themselves. I'll use a basic random forest regressor in all my tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "974cbd1322d5665ec80e3d06c439b617ec55fb68"
   },
   "outputs": [],
   "source": [
    "# Put all features into one dataframe (i.e. aggregate, timeseries, components)\n",
    "feature_df = pd.concat([components_df, aggregate_df, dense_ae_df, classifier_df], axis=1).fillna(0)\n",
    "\n",
    "# Go through each feature\n",
    "results = []\n",
    "for col in tqdm(feature_df.columns):\n",
    "    \n",
    "    # Get the column values in training\n",
    "    X = feature_df.iloc[train_idx][col].values.reshape(-1, 1)\n",
    "    \n",
    "    # Get CV scores\n",
    "    scores = cross_val_score(\n",
    "        ExtraTreesRegressor(n_estimators=30),\n",
    "        X, y,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=10\n",
    "    )\n",
    "    scores = np.sqrt(-scores)\n",
    "    for score in scores:\n",
    "        results.append({'feature': col, 'score': score, 'mean_score': np.mean(scores)})\n",
    "        \n",
    "# Put results in dataframe\n",
    "results = pd.DataFrame(results).sort_values('mean_score')\n",
    "\n",
    "# Only get subset of features for plotting\n",
    "results = results[results.mean_score < np.sort(np.unique(results.mean_score))[100]]\n",
    "\n",
    "# Create plot of scores\n",
    "_, axes = plt.subplots(1, 1, figsize=(20, 5))\n",
    "sns.barplot(x='feature', y='score', data=results, ax=axes)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15439bb33c7954ac79c4a58cfcfe9735b5c278df"
   },
   "source": [
    "I'm pretty sure one has to be careful with some of these features, as they may be overfitting the training data. I'll not go into that analysis in this notebook though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "526a42c5d3ed3802ce30a2bb33637a456c776ce7"
   },
   "source": [
    "## 2.2. Feature Combination Testing\n",
    "Feature combinations can be tested and evaluated in a myriad of ways, but when we are looking at small datasets like in this case, I especially like to use forward/backward feature selection algorithms. So I'll start out with those, and then see how things go - mlxtend comes with a nice package for performing these sequential feature selections, see [here](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)\n",
    "\n",
    "### 2.2.1. Sequential Floating Forward Selection\n",
    "In floating forward selection we first attempt a regression one feature at a time, and then we pick the best one. Afterwards we try combinations of this first feature with all of the other features one at a time, and pick the best combination, and we do this till a specified threshold in number of features are chosen. Floating refers to the fact that when we have 3 or more features in our \"chosen\" feature set, we also try removing each of these features from the set to see if that increases the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9338192d56974f6677ed51649559ac7eb0f1e681",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create forward feature selector\n",
    "selector = SFS(\n",
    "    ExtraTreesRegressor(n_estimators=30),\n",
    "    k_features=(1,15),\n",
    "    forward=True,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10,\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit model and get best features\n",
    "selector.fit(feature_df.values[train_idx], y)\n",
    "\n",
    "# Plot results\n",
    "results = []\n",
    "current_features = []\n",
    "for step, info in selector.subsets_.items():\n",
    "\n",
    "    # What was added / removed on this iteration\n",
    "    added_feature = [i for i in info['feature_idx'] if i not in current_features][0]\n",
    "    removed_feature = [i for i in current_features if i not in info['feature_idx']]    \n",
    "    \n",
    "    # Update book-keeping\n",
    "    current_features.append(added_feature)\n",
    "    \n",
    "    # Save for plotting\n",
    "    label = f\"Added {feature_df.columns[added_feature]}\"\n",
    "    if removed_feature:\n",
    "        label += f\". Removed {feature_df.columns[removed_feature[0]]}\"\n",
    "        current_features.remove(removed_feature[0])\n",
    "    scores = np.sqrt(-info['cv_scores'])\n",
    "    for score in scores:\n",
    "        results.append({'label': label, 'score': score, 'mean_score': np.mean(scores)})\n",
    "        \n",
    "# Put results in dataframe\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Create plot of scores\n",
    "_, axes = plt.subplots(1, 1, figsize=(20, 5))\n",
    "sns.barplot(x='label', y='score', data=results, ax=axes)\n",
    "axes.set_ylim((results.score.min(), results.score.max()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d535ff4956b603f14ddd10575a583fc8805e57c",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
